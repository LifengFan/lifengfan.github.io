<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Publications</title>
  <meta name="description" content="Homepage for Lifeng Fan.">

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <!-- <link rel="stylesheet" id="bulma" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.6.0/css/bulma.min.css" /> -->
  <link rel="stylesheet" id="bulma" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css" />
  <link rel="stylesheet" type="text/css" href="/styles/base.css">
  <link rel="stylesheet" type="text/css" href="/styles/academicons.min.css">

  <link rel="canonical" href="https://lifengfan.github.io/publications/">
</head>

  <body>
    <section class="hero is-fullheight">
      <div class="hero-head">
        <nav class="navbar" role="navigation">
  <div class="navbar-brand">
    <a class="navbar-item" href="/">
      <strong>Lifeng Fan (范丽凤)</strong>
    </a>

    <div class="navbar-burger" data-target="navbar-main">
      <span></span>
      <span></span>
      <span></span>
    </div>
  </div>

  <div class="navbar-menu" id="navbar-main">
    <div class="navbar-start">
      <!-- navbar items -->
      <a href="/" class="navbar-item">
        About
      </a>
      <a class="navbar-item" href="/publications/">
        Publications
      </a>
      <a class="navbar-item" href="/projects/">
        Projects
      </a>
      <a class="navbar-item" href="/files/resume_lifengfan.pdf">
        Resume
      </a>
      <a class="navbar-item" href="/fun/">
        Fun stuff
      </a>
      <a class="navbar-item" href="">
        Blog
      </a>
    </div>

    <div class="navbar-end">
      <!-- navbar items -->
      <div class="navbar-item">
        <a href="mailto:lfan@ucla.edu" class="button is-white">
          <i class="fa fa-lg fa-envelope-o" aria-hidden="true"></i>
        </a>
        <a href="https://scholar.google.com/citations?user=WvdVTmIAAAAJ&hl=en" class="button is-white">
          <i class="ai ai-lg ai-google-scholar-square" aria-hidden="true"></i>
        </a>
        <a href="https://github.com/LifengFan" class="button is-white">
          <i class="fa fa-lg fa-github" aria-hidden="true"></i>
        </a>
      </div>
    </div>
  </div>
</nav>

      </div>

      <div class="hero-body">
        <div class="container">
  <article class="media">
    <div class="media-content">
      <div class="content">
        <h1>Publications</h1>
      </div>
    </div>
  </article>

  <article class="media">
    <div class="media-content">
      <div class="content">
        <p>
        <b>LabelDP-Pro: Learning with Label Differential Privacy via Projections</b>.<br>
        <span class="has-text-grey"><small>
            Badih Ghazi, Yangsibo Huang, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, <u>Chiyuan Zhang</u>.<br>
          International Conference on Learning Representations (<b>ICLR</b>), 2024.
          <a class="tag" href="https://openreview.net/pdf?id=JnYaF3vv3G">PDF</a>
        </small></span>
        </p>
      </div>
    </div>
  </article>


		<h1>News</h1>
		<p><font color="red">02/2024: Our paper on smart helping is accepted by CVPR 2024! </font> 
		<p>08/2023: Our paper on video intent reasoning is accepted by ICCV 2023 as oral! 
<!-- 		<p>07/2021: I started to work as a research scientist at Beijing Institute for General Artificial Intelligence (BIGAI).  -->
<!-- 		<p>06/2021: I got my Ph.D. in Statistics.  -->
<!-- 		<p>06/2020: I started to work as a research intern at Facebook Reality Lab. -->
<!-- 		<p>11/2019: I gave a poster presentation at ICCV 2019, Seoul, Korea. -->
<!-- 		<p>09/2019: I gave a poster presentation at MURI 2019, Edinburgh. -->
<!-- 		<p>07/2019: One paper got accepted by ICCV 2019. -->
<!-- 		<p>06/2018: I gave a spotlight presentation at the 4th Vision Meets Cognition workshop, CVPR 2018, Salt Lake City. <br> -->
<!--                 <p>03/2018: I advanced to candidacy. -->
<!--                 <p>02/2018: Our paper got accepted by CVPR 2018. -->
<!--                 <p>06/2017: I was selected as the 2016-2017 Most Promising Computational Statistician by the Statistics Department. -->
<!--                 <p>06/2017: Our CogSci 2017 paper received Computational Modeling Prize in Perception and Action. -->
<!--                 <p>04/2017: One paper was accepted for oral presentation at CogSci 2017. -->
<!--                 <p>09/2016: I started my Ph.D. life at UCLA.  -->

 
                <br/>
		<br/>

		<h1>Preprint</h1>
			

 		<table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/snowwhite.png" alt="" width="100%" class="border">
		   </td> 
		   <td width="80%" valign="center">
		   <papertitle> Queen or Dwarf? Theory of Mind Matters in Communication-dependent and Hidden-Role Environments</papertitle><br/>
		   
		   <a href="", style="color : #000000;">Lixing Niu</a>,
		   <a href="http://fangweizhong.xyz", style="color : #000000;">Fangwei Zhong</a>,
		   <a href="", style="color : #000000;">Haochen Zhao</a>,
		   <a href="https://yisenwang.github.io", style="color : #000000;">Yisen Wang</a>,
	           <a href="", style="color : #000000;">Xue Feng</a>,
		   <a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong><sup>&#8224;</sup></a>
		   </b><br/>	
		   Under review. <br/>
		   <a href="">Paper (Coming soon)</a> 
		   </td>
		   </tr>
	       </table>	
		
                <h1>Publication</h1>

		<table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/ft_iip.png" alt="" width="100%" class="border">
		   </td> 
		   <td width="80%" valign="center">
		   <papertitle>Evaluating and Modeling Social Intelligence: A Comparative Study of Human and AI Capabilities</papertitle><br/>
		   <a href="", style="color : #000000;">Junqi Wang*</a>,
		   <a href="", style="color : #000000;">Chunhui Zhang*</a>,
	           <a href="", style="color : #000000;">Jiapeng Li</a>,	   
	           <a href="https://mayuxi.com", style="color : #000000;">Yuxi Ma</a>,
		   <a href="", style="color : #000000;">Lixing Niu</a>,
		   <a href="", style="color : #000000;">Jiaheng Han</a>,
		   <a href="https://yujiapeng.myfreesites.net/", style="color : #000000;">Yujia Peng<sup>&#8224;</sup></a>
		   <a href="https://yzhu.io", style="color : #000000;">Yixin Zhu<sup>&#8224;</sup></a>
		   <a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong><sup>&#8224;</sup></a>
		   </b><br/>	
		   Under review. <br/>
		   <a href="">Paper (Coming soon)</a> 
		   </td>
		   </tr>
	       </table>	


			
		<table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/symbol_reasoning.png" alt="" width="100%" class="border">
		   </td> 
		   <td width="80%" valign="center">
		   <papertitle>Learning Concept-Based Causal Transitions for Visual Planning</papertitle><br/>
		   <a href="", style="color : #000000;">Yilue Qian</a>,
		   <a href="", style="color : #000000;">Peiyu Yu</a>,
		   <a href="http://www.stat.ucla.edu/~ywu/", style="color : #000000;">Yingnian Wu</a>,
	           <a href="https://yaosu.info", style="color : #000000;">Yao Su</a>,
	           <a href="https://cognn.com", style="color : #000000;">Wei Wang<sup>&#8224;</sup></a>,
		   <a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong><sup>&#8224;</sup></a>
		   </b><br/>	
		   arXiv, 2023. Under review. <br/>
		   <a href="https://browse.arxiv.org/pdf/2310.03325.pdf">Paper</a> 
		   </td>
		   </tr>
	       </table>	

		
		<table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/help_intro.png" alt="" width="100%" class="border">
		   </td> 
		   <td width="80%" valign="center">
		   <papertitle>Smart Help: Strategic Opponent Modeling for Proactive and Adaptive Robot Assistance in Households</papertitle><br/>
		   <a href="https://tongclass.ac.cn/author/zhihao-cao/", style="color : #000000;">Zhihao Cao*</a>,
		   <a href="", style="color : #000000;">Zidong Wang*</a>,
	           <a href="", style="color : #000000;">Siwen Xie</a>,	   
	           <a href="https://liuanji.github.io", style="color : #000000;">Anji Liu</a>,
		   <a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong><sup>&#8224;</sup></a>
		   </b><br/>	
		   IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. <br/>
		   <a href="">Paper (Coming soon)</a> 
		   </td>
		   </tr>
	       </table>	

		
		<table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/iccv23_teaser.png" alt="" width="100%" class="border">
		   </td> 
		   <td width="80%" valign="center">
		   <papertitle>IntentQA: Context-aware Video Intent Reasoning</papertitle><br/>
		   <a href="https://scholar.google.com/citations?user=PbqDF74AAAAJ&hl=en", style="color : #000000;">Jiapeng Li</a>,
		   <a href="https://gr.xjtu.edu.cn/web/pingwei", style="color : #000000;">Ping Wei</a>,
	           <a href="https://winniehan.github.io/#teaching", style="color : #000000;">Wenjuan Han</a>,
	           <a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong><sup>&#8224;</sup></a>
		   </b><br/>	 
		    The IEEE International Conference on Computer Vision (ICCV), 2023. <font color="red"><strong>(Oral)</strong></font>  <br/>
		    <a href="files/iccv23/03185.pdf">Paper</a> / <a href="files/iccv23/03185-supp.pdf">Supp</a> 
<!-- 		     / <a href="files/iccv23/bib.text">Bibtex</a>  -->
		   </td>
		   </tr>
	       </table>	

		<table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/engineering_tongtest.png" alt="" width="100%" class="border">
		   </td> 
		   <td width="80%" valign="center">
		   <papertitle> The Tong Test: Evaluating Artificial General Intelligence Through Dynamic Embodied Physical and Social Interactions</papertitle><br/>
		   <a href="", style="color : #000000;">Yujia Peng</a>,
		   <a href="", style="color : #000000;">Jiaheng Han</a>,
	           <a href="", style="color : #000000;">Zhenliang Zhang</a>,
		   <a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong></a>,
		   <a href="", style="color : #000000;">Tengyu Liu</a>,
	           <a href="", style="color : #000000;">Siyuan Qi</a>,
		   <a href="", style="color : #000000;">Xue Feng</a>,
		   <a href="", style="color : #000000;">Yuxi Ma</a>,
		   <a href="", style="color : #000000;">Yizhou Wang</a>,
		   <a href="", style="color : #000000;">Song-Chun Zhu<sup>&#8224;</sup></a>,        
		   </b><br/>	 
		    Engineering, 2023. <br/>
		    <a href="https://www.sciencedirect.com/science/article/pii/S209580992300293X">Paper</a> 
		   </td>
		   </tr>
	       </table>	
		
		<table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/asi.png" alt="" width="100%" class="border">
		   </td> 
		   <td width="80%" valign="center">
		   <papertitle>Artificial Social Intelligence: A Comparative and Holistic View</papertitle><br/>
		   <a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong></a>,
		   <a href="https://mjtsu.github.io/", style="color : #000000;">Manjie Xu</a>,
		   <a href="https://tongclass.ac.cn/author/zhihao-cao/", style="color : #000000;">Zhihao Cao</a>,
	           <a href="https://yzhu.io/", style="color : #000000;">Yixin Zhu<sup>&#8224;</sup></a>,
	           <a href="http://www.stat.ucla.edu/~sczhu/", style="color : #000000;">Song-Chun Zhu<sup>&#8224;</sup></a>
		   </b><br/>	 
		    CAAI Artificial Intelligence Research, 2022. <br/>
		    <a href="https://www.sciopen.com/article/10.26599/AIR.2022.9150010">Paper</a> <br/>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href=https://news.sciencenet.cn/htmlpaper/2023/3/202332216433847780313.shtm target=_blank rel=noopener>中国科学报/科学网1</a> /
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href=https://news.sciencenet.cn/htmlpaper/2023/4/2023461854624280764.shtm target=_blank rel=noopener>中国科学报/科学网2</a> /
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href=https://news.pku.edu.cn/jxky/2ce744574d0f4dab8d238d5c75e14efb.htm target=_blank rel=noopener>北大新闻网</a> /
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.ai.pku.edu.cn/info/1053/2484.htm target=_blank rel=noopener>北大AI院官网</a> /
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href=https://mp.weixin.qq.com/s/Bx29E64aO2GhXi6WHd7WKA target=_blank rel=noopener>北大科研官微</a> /
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href=https://mp.weixin.qq.com/s/czqIBh1HHsFg6609VtBXoQ target=_blank rel=noopener>北大新工科官微</a> /
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href=https://mp.weixin.qq.com/s/VgA_dJ8rR__vziR4XJhvUg target=_blank rel=noopener>北大AI院官微</a></div></div><div class="view-list view-list-item"><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
	
		   </td>
		   </tr>
	       </table>	



		
		
	       <table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/pictionary.gif" alt="" width="80%" class="border">
		   </td> 
		   <td width="80%" valign="center">
		   <papertitle>Emergent Graphical Conventions in a Visual Communication Game</papertitle><br/>
		   <a href="https://janetalready.github.io/", style="color : #000000;">Shuwen Qiu</a>*,
		   <a href="https://siruixie.com/", style="color : #000000;">Sirui Xie</a>*,
	           <a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong></a>,
	           <a href="http://www.stat.ucla.edu/~taogao/", style="color : #000000;">Tao Gao</a>,
	           <a href="https://www.jsjoo.com/", style="color : #000000;">Jungseock Joo</a>,
	           <a href="http://www.stat.ucla.edu/~sczhu/", style="color : #000000;">Song-Chun Zhu</a>, 
		   <a href="https://yzhu.io/", style="color : #000000;">Yixin Zhu<sup>&#8224;</sup></a>
		   </b><br/>	    
<!-- 		   * Equal contributions  <br />    -->
		    Conference on Neural Information Processing Systems (NeurIPS), 2022. <br/>
		    <a href="https://arxiv.org/abs/2111.14210">Paper</a> /
		    <a href="https://sites.google.com/view/emergent-graphical-conventions">Web</a><br/>	
		   </td>
		   </tr>
	       </table>	
		
	      <table>
		<table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/thesis/ucla_logo.png" alt="" width="100%" class="border">
		   </td> 
		   <td width="80%" valign="center">
		   <papertitle>A Hierarchical Computational Framework for Social Interaction Understanding: 
			   Interactiveness, Shared Attention, Gaze Communication and Triadic Belief Dynamics</papertitle><br/>
		    <a href="https://escholarship.org/content/qt7mn110r0/qt7mn110r0.pdf">Ph.D. Dissertation, 2021.</a> 
		   </td>
		   </tr>
	       </table>	
	
	      <table>
		<table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/acl-f/arc.png" alt="" width="100%" class="border">
		   </td> 
		   <td width="80%" valign="center">
		   <papertitle>GRICE: A Grammar-based Dataset for Recovering Implicature and Conversational rEasoning</papertitle><br/>
		   <a href="http://web.cs.ucla.edu/~zilongzheng/", style="color : #000000;">Zilong Zheng</a>,
		   <a href="https://janetalready.github.io/", style="color : #000000;">Shuwen Qiu</a>,
	           <a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong></a>,
		   <a href="https://yzhu.io/", style="color : #000000;">Yixin Zhu</a>,
	           <a href="http://www.stat.ucla.edu/~sczhu/", style="color : #000000;">Song-Chun Zhu<sup>&#8224;</sup></a> <br/>    
	            ACL-Findings, 2021.<br/>
		    <a href="https://aclanthology.org/2021.findings-acl.182.pdf">Paper</a> /
		    <a href="https://zilongzheng.github.io/Grice/">Project</a> 
 <!-- 	            <a href="files/acl-f/cite.bib">Bibtex</a><br/>	 -->
		   </td>
		   </tr>
	       </table>		
		
	      <table>
		<table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/cvpr21/motivation.png" alt="belief dynamics" width="100%" class="border">
		   </td> 
		   <td width="80%" valign="center">
		   <papertitle>Learning Triadic Belief Dynamics in Nonverbal Communication from Videos</papertitle><br/>
	           <a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong></a>*,
                   <a href="https://janetalready.github.io/", style="color : #000000;">Shuwen Qiu</a>*,
	           <a href="http://web.cs.ucla.edu/~zilongzheng/", style="color : #000000;">Zilong Zheng</a>,
	           <a href="http://www.stat.ucla.edu/~taogao/", style="color : #000000;">Tao Gao</a>,
	           <a href="http://www.stat.ucla.edu/~sczhu/", style="color : #000000;">Song-Chun Zhu</a>,
	           <a href="https://yzhu.io/", style="color : #000000;">Yixin Zhu<sup>&#8224;</sup></a>
		    </b><br/>	    
<!-- 		   * Equal contributions  <br /> -->
	            IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. <font color="red"><strong>(Oral)</strong></font> <br/>
		    <a href="files/cvpr21/TBD_paper.pdf">Paper</a> /
	            <a href="files/cvpr21/TBD_supp.pdf">Supp</a> /
	            <a href="https://github.com/LifengFan/Triadic-Belief-Dynamics">Code</a> /
		    <a href="https://docs.google.com/forms/d/e/1FAIpQLSe3v-qopGWjx3ZcrCzp7ReRf7VadBuVMhMXCsMe1z3qFVcGvA/viewform?usp=pp_url">Dataset</a> /
		    <a href="https://www.dropbox.com/s/nqai1z32bi66zuy/04411-video.mp4?dl=0">Demo</a>   <br/>
<!-- 	            <a href="files/cvpr21/triadic_belief_dynamics.bib">Bibtex</a><br/>	 -->
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mittrchina.com/news/detail/9677 target=_blank rel=noopener>MIT科技评论</a> /
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.sohu.com/a/473511303_354973 target=_blank rel=noopener>DeepTech</a></div></div><div class="view-list view-list-item"><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
	
			    
		   </td>
		   </tr>
	       </table>
	
		<table>
		<table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/dark/motivation.png" alt="dark matter" width="100%" class="border">
		   </td> 
		   <td width="80%" valign="center">
		   <papertitle>Dark, Beyond Deep: A Paradigm Shift to Cognitive AI with Human-like Commonsense</papertitle><br/>
	            <a href="https://yzhu.io/", style="color : #000000;">Yixin Zhu<sup>&#8224;</sup></a>,
	            <a href="http://www.stat.ucla.edu/~taogao/", style="color : #000000;">Tao Gao</a>,
	            <a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong></a>,
		    <a href="https://siyuanhuang.com/", style="color : #000000;">Siyuan Huang</a>,
	            <a href="http://www.mjedmonds.com/", style="color : #000000;">Mark Edmonds</a>,
		    <a href="https://liuhx111.github.io/", style="color : #000000;">Hangxin Liu</a>,
	            <a href="https://fen9.github.io/", style="color : #000000;">Feng Gao</a>,
		    <a href="http://wellyzhang.github.io/", style="color : #000000;">Chi Zhang</a>, 
		    <a href="http://web.cs.ucla.edu/~syqi/", style="color : #000000;">Siyuan Qi</a>,
	            <a href="http://www.stat.ucla.edu/~ywu/", style="color : #000000;">Yingnian Wu</a>,
		    <a href="https://web.mit.edu/cocosci/josh.html", style="color : #000000;">Joshua B. Tenenbaum</a>,   
	            <a href="http://www.stat.ucla.edu/~sczhu/", style="color : #000000;">Song-Chun Zhu</a>
		    </b><br/>	    
	            Engineering, Special Issue on Artificial Intelligence, 2020. <br />
		    <a href="files/dark/dark.pdf">Paper</a> 
<!-- 	            <a href="files/dark/dark.bib">Bibtex</a><br/>	 -->
		   </td>
		   </tr>
	       </table>
	
	
	        <table>
		<table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/icra20/teaser.png" alt="Sally-Anne" width="100%" class="border">
		   </td>
		   <td width="80%" valign="center">
		   <papertitle>Understanding False-Belief by Joint Inference of Object States, Robot Knowledge, and Human (False-)Beliefs</papertitle><br/>
	            <a href="http://www.stat.ucla.edu/~taoyuan/", style="color : #000000;">Tao Yuan</a>,
	            <a href="https://liuhx111.github.io/", style="color : #000000;">Hangxin Liu</a>,
	            <a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong></a>,
		    <a href="http://web.cs.ucla.edu/~zilongzheng/", style="color : #000000;">Zilong Zheng</a>,
	            <a href="http://www.stat.ucla.edu/~taogao/", style="color : #000000;">Tao Gao</a>,	   
		    <a href="https://yzhu.io/", style="color : #000000;">Yixin Zhu</a>,   
	            <a href="http://www.stat.ucla.edu/~sczhu/", style="color : #000000;">Song-Chun Zhu<sup>&#8224;</sup></a> 
		    </b><br/>	
		    The IEEE International Conference on Robotics and Automation (ICRA), 2020. <br />
		    <a href="files/icra20/icra.pdf">Paper</a>  /
		    <a href="https://vimeo.com/391734593">Demo</a> 
<!--      	            <a href="files/icra20/icra.bib">Bibtex</a><br/>	 -->
		   </td>
		   </tr>
	        </table>
	
	
		<table>
		<table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/iccv19/iccv19_teaser.jpg" alt="ICCV19" width="100%" class="border">
		   </td>
		   <td width="80%" valign="center">
		   <papertitle>Understanding Human Gaze Communication by Spatio-Temporal Graph Reasoning</papertitle><br/>
	            <a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong></a>*,
		    <a href="https://sites.google.com/view/wenguanwang/", style="color : #000000;">Wenguan Wang</a>*,
		    <a href="https://siyuanhuang.com/", style="color : #000000;">Siyuan Huang</a>,
		    Xinyu Tang,
	            <a href="http://www.stat.ucla.edu/~sczhu/", style="color : #000000;">Song-Chun Zhu<sup>&#8224;</sup></a>
		    </b><br/>
<!-- 		    * Equal contributions  <br /> -->
		    The IEEE International Conference on Computer Vision (ICCV), 2019. <br />
		    <a href="files/iccv19/ICCV19_Gaze_Communication.pdf">Paper</a> /
<!-- 		    <a href="https://github.com/LifengFan/Human-Gaze-Communication">Project</a> / -->
		    <a href="https://github.com/LifengFan/Human-Gaze-Communication">Code</a> 
<!--      	            <a href="files/iccv19/gaze_communication.bib">Bibtex</a><br/>	    -->
		   </td>
		   </tr>
	       </table>
	
		<table>
		<table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/cvpr18/cvpr18_teaser.png" alt="CVPR18" width="100%" class="border">
		   </td>
		   <td width="80%" valign="center">
		   <papertitle>Inferring Shared Attention in Social Scene Videos</papertitle><br/>
	            <a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong></a>*,
	            <a href="https://yixchen.github.io/", style="color : #000000;">Yixin Chen</a>*,
		    <a href="https://scholar.google.com/citations?user=1OQBtdcAAAAJ&hl=en", style="color : #000000;">Ping Wei<sup>&#8224;</sup></a>,
		    <a href="https://sites.google.com/view/wenguanwang", style="color : #000000;">Wenguan Wang</a>,
		    <a href="http://www.stat.ucla.edu/~sczhu/", style="color : #000000;">Song-Chun Zhu</a>
		    </b><br/>
<!-- 		    * Equal contributions  <br /> -->
		    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. <br />
		  <a href="files/cvpr18/CVPR18_SharedAttention.pdf">Paper</a> /
<!-- 		  <a href="https://github.com/LifengFan/Shared-Attention"> Project </a> / -->
		  <a href="https://github.com/LifengFan/Shared-Attention"> Code </a> 
		<!--		  <a href=> Code </a> /-->
<!-- 		  <a href="files/cvpr18/shared_attention.bib">Bibtex</a>  <br />  -->
		   </td>
		   </tr>
	       </table>
		

	       <table>
		<table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/cogsci17/topics_teaser.jpg" alt="Topics" width="100%" class="border">
		   </td>
		   <td width="80%" valign="center">
		   <papertitle>Perception of Human Interaction Based on Motion Trajectories: from Aerial Videos to Decontextualized Animations</papertitle><br/>
	           <a href="https://www.tshu.io/", style="color : #000000;">Tianmin Shu</a>*<sup>&#8224;</sup>,
		   <a href="https://yujiapeng.com/", style="color : #000000;">Yujia Peng</a>*,
		   <a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong></a>,
	           <a href="http://cvl.psych.ucla.edu/", style="color : #000000;">Hongjing Lu</a>,
	           <a href="http://www.stat.ucla.edu/~sczhu/", style="color : #000000;">Song-Chun Zhu</a>
		   </b><br/>
<!-- 		   * Equal contributions  <br /> -->
		   Topics in Cognitive Science, 10(1): 225 - 241, 2018. <br />
		   <a href="files/cogsci17/topics.pdf">Paper</a> 
<!-- 		   <a href="files/cogsci17/topics.bib">Bibtex</a><br/> -->
		   </td>
		   </tr>
	       </table>

	       <table>
		<table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/cogsci17/cogsci17_teaser.png" alt="Topics" width="100%" class="border">
		   </td>
		   <td width="80%" valign="center">
		   <papertitle>Inferring Human Interaction from Motion Trajectories in Aerial Videos</papertitle><br/>
	           <a href="https://www.tshu.io/", style="color : #000000;">Tianmin Shu</a>*,
	           <a href="https://yujiapeng.com/", style="color : #000000;">Yujia Peng</a>*,
		   <a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong></a>,
		   <a href="http://cvl.psych.ucla.edu/", style="color : #000000;">Hongjing Lu</a>,
		   <a href="http://www.stat.ucla.edu/~sczhu/", style="color : #000000;">Song-Chun Zhu<sup>&#8224;</sup></a>
		    </b><br/>
<!-- 		   * Equal contributions  <br /> -->
		   the 39th Annual Meeting of the Cognitive Science Society (CogSci), 2017.  <br/>
		   <font color="red"><strong>Computational Modeling Prize</strong></font> <br/>
			
		   <a href="files/cogsci17/cogsci.pdf">Paper</a> /
                   <a href="https://www.tshu.io/HeiderSimmel/CogSci17/"> Project </a> 
<!-- 		   <a href="files/cogsci17/cogsci.bib">Bibtex</a>  <br /> -->
		   </td>
		   </tr>
	       </table>


      </div>

      <div class="hero-foot">
        <footer class="footer">
  <div class="container">
    <div class="content has-text-right is-size-7">
      <p>
      Created &amp; maintained by pluskid, with <a href="https://jekyllrb.com/">jekyll</a> and
      <a href="http://bulma.io/">bulma</a>.
      Content licensed <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY NC SA 4.0</a>.
      </p>
    </div>
  </div>
</footer>

<script async type="text/javascript" src="/javascript/bulma.js"></script>

      </div>
    </section>
  </body>
</html>
