- title: "Queen or Dwarf? Theory of Mind Matters in Communication-dependent and Hidden-Role Environments"
  authors: 
    - name: Lixing Niu
    - name: Fangwei Zhong
      website: 
    - name: Haochen Zhao
    - name: Yisen Wang
    - name: Xue Feng
    - name: <strong>Lifeng Fan†</strong>
      website: https://lifengfan.github.io
  tags:
    - tag: Under Review


- title: "Evaluating and Modeling Social Intelligence: A Comparative Study of Human and AI Capabilities"
  authors:
    - name: Junqi Wang*
    - name: Chunhui Zhang*
    - name: Jiapeng Li
    - name: Yuxi Ma
    - name: Lixing Niu
    - name: Jiaheng Han
    - name: Yujia Peng†
    - name: Yixin Zhu†
    - name: Lifeng Fan†

  tags:
    - tag: Under Review
    - tag: Paper (Coming soon)



- title: "Learning Concept-Based Causal Transitions for Visual Planning"
  authors:
    - name: Yilue Qian
    - name: Peiyu Yu
    - name: Yingnian Wu
    - name: Yao Su
    - name: Wei Wang†
    - name: Lifeng Fan†

  year: 2023
  tags:
    - tag: Under Review
    - tag: arXiv
      link: https://browse.arxiv.org/pdf/2310.03325.pdf


- title: "Smart Help: Strategic Opponent Modeling for Proactive and Adaptive Robot Assistance in Households"
  authors:
    - name: Zhihao Cao*
    - name: Zidong Wang*
    - name: Siwen Xie
    - name: Anji Liu
      website: https://liuanji.github.io
    - name: <strong>Lifeng Fan</strong>†
      website: https://lifengfan.github.io/

  year: 2024
  tags:
    - tag: CVPR
    - tag: Paper (Coming soon)

- title: "IntentQA: Context-aware Video Intent Reasoning"
  authors:
    - name: Jiapeng Li
      website: https://scholar.google.com/citations?user=PbqDF74AAAAJ&hl=en
    - name: Ping Wei
      website: https://gr.xjtu.edu.cn/web/pingwei
    - name: Wenjuan Han
    - name: <strong>Lifeng Fan</strong>†
      website: https://lifengfan.github.io/

  year: 2023
  tags:
    - tag: ICCV
    - tag: Paper
      link: assets/files/iccv23/03185.pdf
    - tag: Supp
      link: assets/files/iccv23/03185-supp.pdf


- title: "The Tong Test: Evaluating Artificial General Intelligence Through Dynamic Embodied Physical and Social Interactions"
  authors:
    - name: Yujia Peng
    - name: Jiaheng Han
    - name: Zhenliang Zhang
    - name: <strong>Lifeng Fan</strong>
      website: https://lifengfan.github.io/
    - name: Tengyu Liu
    - name: Siyuan Qi
    - name: Xue Feng
    - name: Yuxi Ma
    - name: Yizhou Wang
    - name: Song-Chun Zhu†

  year: 2023
  tags:
    - tag: Engineer
    - tag: Paper
      link: https://www.sciencedirect.com/science/article/pii/S209580992300293X



- title: "Artificial Social Intelligence: A Comparative and Holistic View"
  authors:
    - name: <strong>Lifeng Fan</strong>
      website: https://lifengfan.github.io/
    - name: Manjie Xu
    - name: Zhihao Cao
    - name: Yixin Zhu†
      website: https://yzhu.io/
    - name: Song-Chun Zhu†
      website: http://www.stat.ucla.edu/~sczhu/

  year: 2022
  tags:
    - tag: CAAI Artificial Intelligence Research
    - tag: Paper
      link: https://www.sciopen.com/article/10.26599/AIR.2022.9150010
    - tag: 中国科学报/科学网1
      link: https://news.sciencenet.cn/htmlpaper/2023/3/202332216433847780313.shtm
    - tag: 中国科学报/科学网2
      link: https://news.sciencenet.cn/htmlpaper/2023/4/2023461854624280764.shtm
    - tag: 北大新闻网
      link: https://news.pku.edu.cn/jxky/2ce744574d0f4dab8d238d5c75e14efb.htm
    - tag: 北大AI院官网
      link: https://www.ai.pku.edu.cn/info/1053/2484.htm
    - tag: 北大科研官微
      link: https://mp.weixin.qq.com/s/Bx29E64aO2GhXi6WHd7WKA
    - tag: 北大新工科官微
      link: https://mp.weixin.qq.com/s/czqIBh1HHsFg6609VtBXoQ
    - tag: 北大AI院官微
      link: https://mp.weixin.qq.com/s/VgA_dJ8rR__vziR4XJhvUg

  
  
  
  <table width="100%" border="0" align="top" cellpadding="10">
  <tr>
  <td width="20%" valign="center">
  <img src="files/pictionary.gif" alt="" width="80%" class="border">
  </td>
  <td width="80%" valign="center">
  <papertitle>Emergent Graphical Conventions in a Visual Communication Game</papertitle><br/>
<a href="https://janetalready.github.io/", style="color : #000000;">Shuwen Qiu</a>*,
<a href="https://siruixie.com/", style="color : #000000;">Sirui Xie</a>*,
  <a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong></a>,
  <a href="http://www.stat.ucla.edu/~taogao/", style="color : #000000;">Tao Gao</a>,
  <a href="https://www.jsjoo.com/", style="color : #000000;">Jungseock Joo</a>,
  <a href="http://www.stat.ucla.edu/~sczhu/", style="color : #000000;">Song-Chun Zhu</a>,
<a href="https://yzhu.io/", style="color : #000000;">Yixin Zhu<sup>&#8224;</sup></a>
  </b><br/>
  <!-- 		   * Equal contributions  <br />    -->
  Conference on Neural Information Processing Systems (NeurIPS), 2022. <br/>
  <a href="https://arxiv.org/abs/2111.14210">Paper</a> /
  <a href="https://sites.google.com/view/emergent-graphical-conventions">Web</a><br/>
  </td>
  </tr>
  </table>
  
  <table>
  <table width="100%" border="0" align="top" cellpadding="10">
  <tr>
  <td width="20%" valign="center">
  <img src="files/thesis/ucla_logo.png" alt="" width="100%" class="border">
  </td>
  <td width="80%" valign="center">
<papertitle>A Hierarchical Computational Framework for Social Interaction Understanding:
  Interactiveness, Shared Attention, Gaze Communication and Triadic Belief Dynamics</papertitle><br/>
  <a href="https://escholarship.org/content/qt7mn110r0/qt7mn110r0.pdf">Ph.D. Dissertation, 2021.</a>
  </td>
  </tr>
  </table>
  
  <table>
  <table width="100%" border="0" align="top" cellpadding="10">
  <tr>
  <td width="20%" valign="center">
  <img src="files/acl-f/arc.png" alt="" width="100%" class="border">
  </td>
  <td width="80%" valign="center">
<papertitle>GRICE: A Grammar-based Dataset for Recovering Implicature and Conversational rEasoning</papertitle><br/>
<a href="http://web.cs.ucla.edu/~zilongzheng/", style="color : #000000;">Zilong Zheng</a>,
<a href="https://janetalready.github.io/", style="color : #000000;">Shuwen Qiu</a>,
  <a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong></a>,
<a href="https://yzhu.io/", style="color : #000000;">Yixin Zhu</a>,
  <a href="http://www.stat.ucla.edu/~sczhu/", style="color : #000000;">Song-Chun Zhu<sup>&#8224;</sup></a> <br/>
    ACL-Findings, 2021.<br/>
  <a href="https://aclanthology.org/2021.findings-acl.182.pdf">Paper</a> /
  <a href="https://zilongzheng.github.io/Grice/">Project</a>
  <!-- 	            <a href="files/acl-f/cite.bib">Bibtex</a><br/>	 -->
  </td>
  </tr>
  </table>
  
  <table>
  <table width="100%" border="0" align="top" cellpadding="10">
  <tr>
  <td width="20%" valign="center">
  <img src="files/cvpr21/motivation.png" alt="belief dynamics" width="100%" class="border">
  </td>
  <td width="80%" valign="center">
  <papertitle>Learning Triadic Belief Dynamics in Nonverbal Communication from Videos</papertitle><br/>
<a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong></a>*,
  <a href="https://janetalready.github.io/", style="color : #000000;">Shuwen Qiu</a>*,
<a href="http://web.cs.ucla.edu/~zilongzheng/", style="color : #000000;">Zilong Zheng</a>,
<a href="http://www.stat.ucla.edu/~taogao/", style="color : #000000;">Tao Gao</a>,
<a href="http://www.stat.ucla.edu/~sczhu/", style="color : #000000;">Song-Chun Zhu</a>,
<a href="https://yzhu.io/", style="color : #000000;">Yixin Zhu<sup>&#8224;</sup></a>
  </b><br/>
  <!-- 		   * Equal contributions  <br /> -->
  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. <font color="red"><strong>(Oral)</strong></font> <br/>
  <a href="files/cvpr21/TBD_paper.pdf">Paper</a> /
  <a href="files/cvpr21/TBD_supp.pdf">Supp</a> /
  <a href="https://github.com/LifengFan/Triadic-Belief-Dynamics">Code</a> /
  <a href="https://docs.google.com/forms/d/e/1FAIpQLSe3v-qopGWjx3ZcrCzp7ReRf7VadBuVMhMXCsMe1z3qFVcGvA/viewform?usp=pp_url">Dataset</a> /
  <a href="https://www.dropbox.com/s/nqai1z32bi66zuy/04411-video.mp4?dl=0">Demo</a>   <br/>
  <!-- 	            <a href="files/cvpr21/triadic_belief_dynamics.bib">Bibtex</a><br/>	 -->
  <a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mittrchina.com/news/detail/9677 target=_blank rel=noopener>MIT科技评论</a> /
  <a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.sohu.com/a/473511303_354973 target=_blank rel=noopener>DeepTech</a></div></div><div class="view-list view-list-item"><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
  
  
  </td>
  </tr>
  </table>
  
  <table>
  <table width="100%" border="0" align="top" cellpadding="10">
  <tr>
  <td width="20%" valign="center">
  <img src="files/dark/motivation.png" alt="dark matter" width="100%" class="border">
  </td>
  <td width="80%" valign="center">
<papertitle>Dark, Beyond Deep: A Paradigm Shift to Cognitive AI with Human-like Commonsense</papertitle><br/>
  <a href="https://yzhu.io/", style="color : #000000;">Yixin Zhu<sup>&#8224;</sup></a>,
  <a href="http://www.stat.ucla.edu/~taogao/", style="color : #000000;">Tao Gao</a>,
  <a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong></a>,
  <a href="https://siyuanhuang.com/", style="color : #000000;">Siyuan Huang</a>,
    <a href="http://www.mjedmonds.com/", style="color : #000000;">Mark Edmonds</a>,
  <a href="https://liuhx111.github.io/", style="color : #000000;">Hangxin Liu</a>,
    <a href="https://fen9.github.io/", style="color : #000000;">Feng Gao</a>,
  <a href="http://wellyzhang.github.io/", style="color : #000000;">Chi Zhang</a>,
  <a href="http://web.cs.ucla.edu/~syqi/", style="color : #000000;">Siyuan Qi</a>,
    <a href="http://www.stat.ucla.edu/~ywu/", style="color : #000000;">Yingnian Wu</a>,
  <a href="https://web.mit.edu/cocosci/josh.html", style="color : #000000;">Joshua B. Tenenbaum</a>,
    <a href="http://www.stat.ucla.edu/~sczhu/", style="color : #000000;">Song-Chun Zhu</a>
  </b><br/>
  Engineering, Special Issue on Artificial Intelligence, 2020. <br />
  <a href="files/dark/dark.pdf">Paper</a>
  <!-- 	            <a href="files/dark/dark.bib">Bibtex</a><br/>	 -->
  </td>
  </tr>
  </table>
  
  
  <table>
  <table width="100%" border="0" align="top" cellpadding="10">
  <tr>
  <td width="20%" valign="center">
  <img src="files/icra20/teaser.png" alt="Sally-Anne" width="100%" class="border">
  </td>
  <td width="80%" valign="center">
  <papertitle>Understanding False-Belief by Joint Inference of Object States, Robot Knowledge, and Human (False-)Beliefs</papertitle><br/>
<a href="http://www.stat.ucla.edu/~taoyuan/", style="color : #000000;">Tao Yuan</a>,
<a href="https://liuhx111.github.io/", style="color : #000000;">Hangxin Liu</a>,
<a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong></a>,
<a href="http://web.cs.ucla.edu/~zilongzheng/", style="color : #000000;">Zilong Zheng</a>,
  <a href="http://www.stat.ucla.edu/~taogao/", style="color : #000000;">Tao Gao</a>,
<a href="https://yzhu.io/", style="color : #000000;">Yixin Zhu</a>,
  <a href="http://www.stat.ucla.edu/~sczhu/", style="color : #000000;">Song-Chun Zhu<sup>&#8224;</sup></a>
  </b><br/>
  The IEEE International Conference on Robotics and Automation (ICRA), 2020. <br />
  <a href="files/icra20/icra.pdf">Paper</a>  /
  <a href="https://vimeo.com/391734593">Demo</a>
  <!--      	            <a href="files/icra20/icra.bib">Bibtex</a><br/>	 -->
  </td>
  </tr>
  </table>
  
  
  <table>
  <table width="100%" border="0" align="top" cellpadding="10">
  <tr>
  <td width="20%" valign="center">
  <img src="files/iccv19/iccv19_teaser.jpg" alt="ICCV19" width="100%" class="border">
  </td>
  <td width="80%" valign="center">
  <papertitle>Understanding Human Gaze Communication by Spatio-Temporal Graph Reasoning</papertitle><br/>
<a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong></a>*,
<a href="https://sites.google.com/view/wenguanwang/", style="color : #000000;">Wenguan Wang</a>*,
<a href="https://siyuanhuang.com/", style="color : #000000;">Siyuan Huang</a>,
  Xinyu Tang,
<a href="http://www.stat.ucla.edu/~sczhu/", style="color : #000000;">Song-Chun Zhu<sup>&#8224;</sup></a>
  </b><br/>
  <!-- 		    * Equal contributions  <br /> -->
  The IEEE International Conference on Computer Vision (ICCV), 2019. <br />
  <a href="files/iccv19/ICCV19_Gaze_Communication.pdf">Paper</a> /
  <!-- 		    <a href="https://github.com/LifengFan/Human-Gaze-Communication">Project</a> / -->
  <a href="https://github.com/LifengFan/Human-Gaze-Communication">Code</a>
  <!--      	            <a href="files/iccv19/gaze_communication.bib">Bibtex</a><br/>	    -->
  </td>
  </tr>
  </table>
  
  <table>
  <table width="100%" border="0" align="top" cellpadding="10">
  <tr>
  <td width="20%" valign="center">
  <img src="files/cvpr18/cvpr18_teaser.png" alt="CVPR18" width="100%" class="border">
  </td>
  <td width="80%" valign="center">
  <papertitle>Inferring Shared Attention in Social Scene Videos</papertitle><br/>
<a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong></a>*,
<a href="https://yixchen.github.io/", style="color : #000000;">Yixin Chen</a>*,
<a href="https://scholar.google.com/citations?user=1OQBtdcAAAAJ&hl=en", style="color : #000000;">Ping Wei<sup>&#8224;</sup></a>,
<a href="https://sites.google.com/view/wenguanwang", style="color : #000000;">Wenguan Wang</a>,
<a href="http://www.stat.ucla.edu/~sczhu/", style="color : #000000;">Song-Chun Zhu</a>
  </b><br/>
  <!-- 		    * Equal contributions  <br /> -->
  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. <br />
  <a href="files/cvpr18/CVPR18_SharedAttention.pdf">Paper</a> /
  <!-- 		  <a href="https://github.com/LifengFan/Shared-Attention"> Project </a> / -->
  <a href="https://github.com/LifengFan/Shared-Attention"> Code </a>
  <!--		  <a href=> Code </a> /-->
  <!-- 		  <a href="files/cvpr18/shared_attention.bib">Bibtex</a>  <br />  -->
  </td>
  </tr>
  </table>
  
  
  <table>
  <table width="100%" border="0" align="top" cellpadding="10">
  <tr>
  <td width="20%" valign="center">
  <img src="files/cogsci17/topics_teaser.jpg" alt="Topics" width="100%" class="border">
  </td>
  <td width="80%" valign="center">
<papertitle>Perception of Human Interaction Based on Motion Trajectories: from Aerial Videos to Decontextualized Animations</papertitle><br/>
  <a href="https://www.tshu.io/", style="color : #000000;">Tianmin Shu</a>*<sup>&#8224;</sup>,
<a href="https://yujiapeng.com/", style="color : #000000;">Yujia Peng</a>*,
<a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong></a>,
  <a href="http://cvl.psych.ucla.edu/", style="color : #000000;">Hongjing Lu</a>,
  <a href="http://www.stat.ucla.edu/~sczhu/", style="color : #000000;">Song-Chun Zhu</a>
  </b><br/>
  <!-- 		   * Equal contributions  <br /> -->
Topics in Cognitive Science, 10(1): 225 - 241, 2018. <br />
  <a href="files/cogsci17/topics.pdf">Paper</a>
  <!-- 		   <a href="files/cogsci17/topics.bib">Bibtex</a><br/> -->
  </td>
  </tr>
  </table>
  
  <table>
  <table width="100%" border="0" align="top" cellpadding="10">
  <tr>
  <td width="20%" valign="center">
  <img src="files/cogsci17/cogsci17_teaser.png" alt="Topics" width="100%" class="border">
  </td>
  <td width="80%" valign="center">
  <papertitle>Inferring Human Interaction from Motion Trajectories in Aerial Videos</papertitle><br/>
<a href="https://www.tshu.io/", style="color : #000000;">Tianmin Shu</a>*,
<a href="https://yujiapeng.com/", style="color : #000000;">Yujia Peng</a>*,
<a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong></a>,
<a href="http://cvl.psych.ucla.edu/", style="color : #000000;">Hongjing Lu</a>,
<a href="http://www.stat.ucla.edu/~sczhu/", style="color : #000000;">Song-Chun Zhu<sup>&#8224;</sup></a>
  </b><br/>
  <!-- 		   * Equal contributions  <br /> -->
  the 39th Annual Meeting of the Cognitive Science Society (CogSci), 2017.  <br/>
  <font color="red"><strong>Computational Modeling Prize</strong></font> <br/>
  
  <a href="files/cogsci17/cogsci.pdf">Paper</a> /
  <a href="https://www.tshu.io/HeiderSimmel/CogSci17/"> Project </a>
  <!-- 		   <a href="files/cogsci17/cogsci.bib">Bibtex</a>  <br /> -->
  </td>
  </tr>
  </table>