<!DOCTYPE html>
<html>

<head>
<title>Lifeng Fan</title>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<style>
	body {
		font: 14px/1.5 -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
		padding-left: 10%;
		padding-right: 10%;
	}

	header {
		border-bottom: 1px solid gray;
		text-align: center;
	}

	nav {
	    float: left;
	    padding-top: 3%;
	    display: block;
	}

	article {
		overflow: hidden;
		padding-left: 5%;
	}

	a:link, a:visited {
		color: #047DF2;/*#2da38d;#356d63;#3a8c7d;*/
		text-decoration: none;
	}

	strong {
	font-family: Lora;
	font-size: 16px;
	font-weight: 600;
}

</style>
</head>

<body>
    <div height="40" id="header" style="background-color:#002FA7; color: #002FA7">
      <center>
        <table width="1050" height="40" border="0">
          <tr>
		<td halign="center">
			<p align="center"><font size="6"><font color=#FFFFFF>Lifeng Fan</font> </p>
		</td>
	</tr>
        </table>
      </center>
    </div>
	
	
<!-- 	<header class="header">
	  <h1>Lifeng Fan (范丽凤)</h1>
	</header> -->

	<nav class="nav">
		<center>
<!-- 			<a href="map.html"> -->
			<img src="files/fanlifeng_new.JPG" style="height:200px;"></a>
		</center>
		<ul class="fa-ul">
			<li><i class="fa-li fa fa-envelope"></i>
				<a href="mailto:lfan@ucla.edu" style="text-decoration: none">Email</a>
			</li>
			
			<li><i class="fa-li fa fa-github"></i>
				<a href="https://github.com/LifengFan" style="text-decoration: none">Github</a>
			</li>
			
			<li><i class="fa-li fa fa-book"></i>
				<a href="https://scholar.google.com/citations?user=WvdVTmIAAAAJ&hl=en" style="text-decoration: none">Google Scholar</a>
			</li>
			
			<li><i class="fa-li fa fa-file"></i><a href="files/resume_lifengfan.pdf">Resume</a>
			</li>
		</ul>
	</nav>

	<article class="article">
		<h1>About Me</h1>
		<p> I am a research scientist at Beijing Institute for General Artificial Intelligence (BIGAI) now. I pursued my Ph.D. in the Department of Statistics at University of California, Los Angeles (UCLA). 
			I worked at the <a href="https://vcla.stat.ucla.edu/">Center for Vision, Cognition, Learning and Autonomy (VCLA)</a> 
			under the supervision of <a href="http://www.stat.ucla.edu/~sczhu/">Prof. Song-Chun Zhu</a>.
			I received a B.S. in Statistics at Zhejiang University. 
			My research interests include social scene understanding, cognitive modeling, human communication dynamics, social interaction and cooperation, Theory of Mind, computer vision, artificial intelligence, etc.
	        </p>
		
		<p><i> "Nothing in life is to be feared; it is only to be understood."</i>  --Marie Curie </p>
		
		

		<h1>News</h1>
		<p>08/2023: Our paper on video intent reasoning is accepted by ICCV 2023 as oral! 
<!-- 		<p>07/2021: I started to work as a research scientist at Beijing Institute for General Artificial Intelligence (BIGAI).  -->
<!-- 		<p>06/2021: I got my Ph.D. in Statistics.  -->
<!-- 		<p>06/2020: I started to work as a research intern at Facebook Reality Lab. -->
<!-- 		<p>11/2019: I gave a poster presentation at ICCV 2019, Seoul, Korea. -->
<!-- 		<p>09/2019: I gave a poster presentation at MURI 2019, Edinburgh. -->
<!-- 		<p>07/2019: One paper got accepted by ICCV 2019. -->
<!-- 		<p>06/2018: I gave a spotlight presentation at the 4th Vision Meets Cognition workshop, CVPR 2018, Salt Lake City. <br> -->
<!--                 <p>03/2018: I advanced to candidacy. -->
<!--                 <p>02/2018: Our paper got accepted by CVPR 2018. -->
<!--                 <p>06/2017: I was selected as the 2016-2017 Most Promising Computational Statistician by the Statistics Department. -->
<!--                 <p>06/2017: Our CogSci 2017 paper received Computational Modeling Prize in Perception and Action. -->
<!--                 <p>04/2017: One paper was accepted for oral presentation at CogSci 2017. -->
<!--                 <p>09/2016: I started my Ph.D. life at UCLA.  -->




			

		<h1>Preprint</h1>
			
		<table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="" alt="" width="100%" class="border">
		   </td> 
		   <td width="80%" valign="center">
		   <papertitle>Learning Concept-Based Causal Transitions for Visual Planning</papertitle><br/>
		   
		   <a href="", style="color : #000000;">Yilue Qian</a>,
		   <a href="", style="color : #000000;">Peiyu Yu</a>,
	           <a href="", style="color : #000000;">Wei Wang</a>,
		   <a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong></a>,
		   </b><br/>	 
		   </td>
		   </tr>
	       </table>	

		<table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="" alt="" width="100%" class="border">
		   </td> 
		   <td width="80%" valign="center">
		   <papertitle>Strategic Opponent Modeling for Proactive and Adaptive Robot Assistance in Households</papertitle><br/>
		   
		   <a href="", style="color : #000000;">Zidong Wang</a>,
		   <a href="", style="color : #000000;">Zhihao Cao</a>,
	           <a href="", style="color : #000000;">Anji Liu</a>,
		   <a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong></a>
		   </b><br/>	 
		   </td>
		   </tr>
	       </table>	

		<table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="" alt="" width="100%" class="border">
		   </td> 
		   <td width="80%" valign="center">
		   <papertitle> Queen or Dwarf? Theory of Mind Matters in Communication-dependent and Hidden-Role Environments</papertitle><br/>
		   
		   <a href="", style="color : #000000;">Lixing Niu</a>,
		   <a href="", style="color : #000000;">Fangwei Zhong</a>,
		   <a href="", style="color : #000000;">Haochen Zhao</a>,
	           <a href="", style="color : #000000;">Xue Feng</a>,
		   <a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong></a>
		   </b><br/>	 
		   </td>
		   </tr>
	       </table>	
		
 <h1>Publication</h1>

		
		<table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/iccv23_teaser.png" alt="" width="100%" class="border">
		   </td> 
		   <td width="80%" valign="center">
		   <papertitle>IntentQA: Context-aware Video Intent Reasoning</papertitle><br/>
		   <a href="", style="color : #000000;">Jiapeng Li</a>,
		   <a href="", style="color : #000000;">Ping Wei</a>,
	           <a href="", style="color : #000000;">Wenjuan Han</a>,
	           <a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan<sup>&#8224;</sup></strong></a>
		   </b><br/>	 
		    The IEEE International Conference on Computer Vision (ICCV), 2023. <font color="red"><strong>(Oral)</strong></font>  <br/>
		    <a href="files/iccv23/03185.pdf">Paper</a> / <a href="files/iccv23/03185-supp.pdf">Supp</a> 
<!-- 		     / <a href="files/iccv23/bib.text">Bibtex</a>  -->
		   </td>
		   </tr>
	       </table>	

		<table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/engineering_tongtest.png" alt="" width="100%" class="border">
		   </td> 
		   <td width="80%" valign="center">
		   <papertitle> The Tong Test: Evaluating Artificial General Intelligence Through Dynamic Embodied Physical and Social Interactions</papertitle><br/>
		   <a href="", style="color : #000000;">Yujia Peng</a>,
		   <a href="", style="color : #000000;">Jiaheng Han</a>,
	           <a href="", style="color : #000000;">Zhenliang Zhang</a>,
		   <a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong></a>,
		   <a href="", style="color : #000000;">Tengyu Liu</a>,
	           <a href="", style="color : #000000;">Siyuan Qi</a>,
		   <a href="", style="color : #000000;">Xue Feng</a>,
		   <a href="", style="color : #000000;">Yuxi Ma</a>,
		   <a href="", style="color : #000000;">Yizhou Wang</a>,
		   <a href="", style="color : #000000;">Song-Chun Zhu</a>,        
		   </b><br/>	 
		    Engineering, 2023. <br/>
		    <a href="">Paper</a> 
		   </td>
		   </tr>
	       </table>	
		
		<table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/asi.png" alt="" width="100%" class="border">
		   </td> 
		   <td width="80%" valign="center">
		   <papertitle>Artificial Social Intelligence: A Comparative and Holistic View</papertitle><br/>
		   <a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong></a>,
		   <a href="https://mjtsu.github.io/", style="color : #000000;">Manjie Xu</a>,
		   <a href="https://tongclass.ac.cn/author/zhihao-cao/", style="color : #000000;">Zhihao Cao</a>,
	           <a href="https://yzhu.io/", style="color : #000000;">Yixin Zhu</a>,
	           <a href="http://www.stat.ucla.edu/~sczhu/", style="color : #000000;">Song-Chun Zhu</a>
		   </b><br/>	 
		    CAAI Artificial Intelligence Research, 2022. <br/>
		    <a href="https://www.sciopen.com/article/10.26599/AIR.2022.9150010">Paper</a> <br/>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href=https://news.sciencenet.cn/htmlpaper/2023/3/202332216433847780313.shtm target=_blank rel=noopener>中国科学报/科学网1</a> /
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href=https://news.sciencenet.cn/htmlpaper/2023/4/2023461854624280764.shtm target=_blank rel=noopener>中国科学报/科学网2</a> /
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href=https://news.pku.edu.cn/jxky/2ce744574d0f4dab8d238d5c75e14efb.htm target=_blank rel=noopener>北大新闻网</a> /
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.ai.pku.edu.cn/info/1053/2484.htm target=_blank rel=noopener>北大AI院官网</a> /
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href=https://mp.weixin.qq.com/s/Bx29E64aO2GhXi6WHd7WKA target=_blank rel=noopener>北大科研官微</a> /
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href=https://mp.weixin.qq.com/s/czqIBh1HHsFg6609VtBXoQ target=_blank rel=noopener>北大新工科官微</a> /
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href=https://mp.weixin.qq.com/s/VgA_dJ8rR__vziR4XJhvUg target=_blank rel=noopener>北大AI院官微</a></div></div><div class="view-list view-list-item"><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
	
		   </td>
		   </tr>
	       </table>	



		
		
	       <table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/pictionary.gif" alt="" width="80%" class="border">
		   </td> 
		   <td width="80%" valign="center">
		   <papertitle>Emergent Graphical Conventions in a Visual Communication Game</papertitle><br/>
		   <a href="https://janetalready.github.io/", style="color : #000000;">Shuwen Qiu</a>*,
		   <a href="https://siruixie.com/", style="color : #000000;">Sirui Xie</a>*,
	           <a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong></a>,
	           <a href="http://www.stat.ucla.edu/~taogao/", style="color : #000000;">Tao Gao</a>,
	           <a href="http://www.stat.ucla.edu/~sczhu/", style="color : #000000;">Song-Chun Zhu</a>, 
		   <a href="https://yzhu.io/", style="color : #000000;">Yixin Zhu</a>
		   </b><br/>	    
<!-- 		   * Equal contributions  <br />    -->
		    Conference on Neural Information Processing Systems (NeurIPS), 2022. <br/>
		    <a href="https://arxiv.org/abs/2111.14210">Paper</a> /
		    <a href="https://sites.google.com/view/emergent-graphical-conventions">Web</a><br/>	
		   </td>
		   </tr>
	       </table>	
		
	      <table>
		<table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/thesis/ucla_logo.png" alt="" width="100%" class="border">
		   </td> 
		   <td width="80%" valign="center">
		   <papertitle>A Hierarchical Computational Framework for Social Interaction Understanding: 
			   Interactiveness, Shared Attention, Gaze Communication and Triadic Belief Dynamics</papertitle><br/>
		    <a href="https://escholarship.org/content/qt7mn110r0/qt7mn110r0.pdf">Ph.D. Dissertation, 2021.</a> 
		   </td>
		   </tr>
	       </table>	
	
	      <table>
		<table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/acl-f/arc.png" alt="" width="100%" class="border">
		   </td> 
		   <td width="80%" valign="center">
		   <papertitle>GRICE: A Grammar-based Dataset for Recovering Implicature and Conversational rEasoning</papertitle><br/>
		   <a href="http://web.cs.ucla.edu/~zilongzheng/", style="color : #000000;">Zilong Zheng</a>,
		   <a href="https://janetalready.github.io/", style="color : #000000;">Shuwen Qiu</a>,
	           <a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong></a>,
		   <a href="https://yzhu.io/", style="color : #000000;">Yixin Zhu</a>,
	           <a href="http://www.stat.ucla.edu/~sczhu/", style="color : #000000;">Song-Chun Zhu</a>,    
	            ACL-Findings, 2021.<br/>
		    <a href="https://yzhu.io/publication/implicature2021acl-findings/paper.pdf">Paper</a> /
		    <a href="https://zilongzheng.github.io/Grice/">Project</a> /
<!-- 	            <a href="files/acl-f/cite.bib">Bibtex</a><br/>	 -->
		   </td>
		   </tr>
	       </table>		
		
	      <table>
		<table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/cvpr21/motivation.png" alt="belief dynamics" width="100%" class="border">
		   </td> 
		   <td width="80%" valign="center">
		   <papertitle>Learning Triadic Belief Dynamics in Nonverbal Communication from Videos</papertitle><br/>
	           <a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong></a>*,
                   <a href="https://janetalready.github.io/", style="color : #000000;">Shuwen Qiu</a>*,
	           <a href="http://web.cs.ucla.edu/~zilongzheng/", style="color : #000000;">Zilong Zheng</a>,
	           <a href="http://www.stat.ucla.edu/~taogao/", style="color : #000000;">Tao Gao</a>,
	           <a href="http://www.stat.ucla.edu/~sczhu/", style="color : #000000;">Song-Chun Zhu</a>,
	           <a href="https://yzhu.io/", style="color : #000000;">Yixin Zhu</a>
		    </b><br/>	    
<!-- 		   * Equal contributions  <br /> -->
	            IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. <font color="red"><strong>(Oral)</strong></font> <br/>
		    <a href="files/cvpr21/TBD_paper.pdf">Paper</a> /
	            <a href="files/cvpr21/TBD_supp.pdf">Supp</a> /
	            <a href="https://github.com/LifengFan/Triadic-Belief-Dynamics">Code</a> /
		    <a href="https://docs.google.com/forms/d/e/1FAIpQLSe3v-qopGWjx3ZcrCzp7ReRf7VadBuVMhMXCsMe1z3qFVcGvA/viewform?usp=pp_url">Dataset</a> /
		    <a href="https://www.dropbox.com/s/nqai1z32bi66zuy/04411-video.mp4?dl=0">Demo</a> /
<!-- 	            <a href="files/cvpr21/triadic_belief_dynamics.bib">Bibtex</a><br/>	 -->
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.mittrchina.com/news/detail/9677 target=_blank rel=noopener>MIT科技评论</a></div></div><div class="view-list view-list-item"><i class="far fa-file-alt pub-icon" aria-hidden=true></i> / 
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.sohu.com/a/473511303_354973 target=_blank rel=noopener>DeepTech</a></div></div><div class="view-list view-list-item"><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
	
			    
		   </td>
		   </tr>
	       </table>
	
		<table>
		<table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/dark/motivation.png" alt="dark matter" width="100%" class="border">
		   </td> 
		   <td width="80%" valign="center">
		   <papertitle>Dark, Beyond Deep: A Paradigm Shift to Cognitive AI with Human-like Commonsense</papertitle><br/>
	            <a href="https://yzhu.io/", style="color : #000000;">Yixin Zhu</a>,
	            <a href="http://www.stat.ucla.edu/~taogao/", style="color : #000000;">Tao Gao</a>,
	            <a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong></a>,
		    <a href="https://siyuanhuang.com/", style="color : #000000;">Siyuan Huang</a>,
	            <a href="http://www.mjedmonds.com/", style="color : #000000;">Mark Edmonds</a>,
		    <a href="https://liuhx111.github.io/", style="color : #000000;">Hangxin Liu</a>,
	            <a href="https://fen9.github.io/", style="color : #000000;">Feng Gao</a>,
		    <a href="http://wellyzhang.github.io/", style="color : #000000;">Chi Zhang</a>, 
		    <a href="http://web.cs.ucla.edu/~syqi/", style="color : #000000;">Siyuan Qi</a>,
	            <a href="http://www.stat.ucla.edu/~ywu/", style="color : #000000;">Yingnian Wu</a>,
		    <a href="https://web.mit.edu/cocosci/josh.html", style="color : #000000;">Joshua B. Tenenbaum</a>,   
	            <a href="http://www.stat.ucla.edu/~sczhu/", style="color : #000000;">Song-Chun Zhu</a>
		    </b><br/>	    
	            Engineering, Special Issue on Artificial Intelligence, 2020. <br />
		    <a href="files/dark/dark.pdf">Paper</a> /
<!-- 	            <a href="files/dark/dark.bib">Bibtex</a><br/>	 -->
		   </td>
		   </tr>
	       </table>
	
	
	        <table>
		<table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/icra20/teaser.png" alt="Sally-Anne" width="100%" class="border">
		   </td>
		   <td width="80%" valign="center">
		   <papertitle>Understanding False-Belief by Joint Inference of Object States, Robot Knowledge, and Human (False-)Beliefs</papertitle><br/>
	            <a href="http://www.stat.ucla.edu/~taoyuan/", style="color : #000000;">Tao Yuan</a>,
	            <a href="https://liuhx111.github.io/", style="color : #000000;">Hangxin Liu</a>,
	            <a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong></a>,
		    <a href="http://web.cs.ucla.edu/~zilongzheng/", style="color : #000000;">Zilong Zheng</a>,
	            <a href="http://www.stat.ucla.edu/~taogao/", style="color : #000000;">Tao Gao</a>,	   
		    <a href="https://yzhu.io/", style="color : #000000;">Yixin Zhu</a>,   
	            <a href="http://www.stat.ucla.edu/~sczhu/", style="color : #000000;">Song-Chun Zhu</a> 
		    </b><br/>	
		    The IEEE International Conference on Robotics and Automation (ICRA), 2020. <br />
		    <a href="files/icra20/icra.pdf">Paper</a>  /
		    <a href="https://vimeo.com/391734593">Demo</a> /
<!--      	            <a href="files/icra20/icra.bib">Bibtex</a><br/>	 -->
		   </td>
		   </tr>
	        </table>
	
	
		<table>
		<table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/iccv19/iccv19_teaser.jpg" alt="ICCV19" width="100%" class="border">
		   </td>
		   <td width="80%" valign="center">
		   <papertitle>Understanding Human Gaze Communication by Spatio-Temporal Graph Reasoning</papertitle><br/>
	            <a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong></a>*,
		    <a href="https://sites.google.com/view/wenguanwang/", style="color : #000000;">Wenguan Wang</a>*,
		    <a href="https://siyuanhuang.com/", style="color : #000000;">Siyuan Huang</a>,
		    Xinyu Tang,
	            <a href="http://www.stat.ucla.edu/~sczhu/", style="color : #000000;">Song-Chun Zhu</a>
		    </b><br/>
<!-- 		    * Equal contributions  <br /> -->
		    The IEEE International Conference on Computer Vision (ICCV), 2019. <br />
		    <a href="files/iccv19/ICCV19_Gaze_Communication.pdf">Paper</a> /
			<a href="https://github.com/LifengFan/Human-Gaze-Communication">Project</a> /
<!--      	            <a href="files/iccv19/gaze_communication.bib">Bibtex</a><br/>	    -->
		   </td>
		   </tr>
	       </table>
	
		<table>
		<table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/cvpr18/cvpr18_teaser.png" alt="CVPR18" width="100%" class="border">
		   </td>
		   <td width="80%" valign="center">
		   <papertitle>Inferring Shared Attention in Social Scene Videos</papertitle><br/>
	            <a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong></a>*,
	            <a href="https://yixchen.github.io/", style="color : #000000;">Yixin Chen</a>*,
		    <a href="https://scholar.google.com/citations?user=1OQBtdcAAAAJ&hl=en", style="color : #000000;">Ping Wei</a>,
		    <a href="https://sites.google.com/view/wenguanwang", style="color : #000000;">Wenguan Wang</a>,
		    <a href="http://www.stat.ucla.edu/~sczhu/", style="color : #000000;">Song-Chun Zhu</a>
		    </b><br/>
<!-- 		    * Equal contributions  <br /> -->
		    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. <br />
		  <a href="files/cvpr18/CVPR18_SharedAttention.pdf">Paper</a> /
		  <a href="https://github.com/LifengFan/Shared-Attention"> Project </a> /
		<!--		  <a href=> Code </a> /-->
<!-- 		  <a href="files/cvpr18/shared_attention.bib">Bibtex</a>  <br />  -->
		   </td>
		   </tr>
	       </table>
		

	       <table>
		<table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/cogsci17/topics_teaser.jpg" alt="Topics" width="100%" class="border">
		   </td>
		   <td width="80%" valign="center">
		   <papertitle>Perception of Human Interaction Based on Motion Trajectories: from Aerial Videos to Decontextualized Animations</papertitle><br/>
	           <a href="https://www.tshu.io/", style="color : #000000;">Tianmin Shu</a>*,
		   <a href="https://yujiapeng.com/", style="color : #000000;">Yujia Peng</a>*,
		   <a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong></a>,
	           <a href="http://cvl.psych.ucla.edu/", style="color : #000000;">Hongjing Lu</a>,
	           <a href="http://www.stat.ucla.edu/~sczhu/", style="color : #000000;">Song-Chun Zhu</a>
		   </b><br/>
<!-- 		   * Equal contributions  <br /> -->
		   Topics in Cognitive Science, 10(1): 225 - 241, 2018. <br />
		   <a href="files/cogsci17/topics.pdf">Paper</a> /
		   <a href="files/cogsci17/topics.bib">Bibtex</a><br/>
		   </td>
		   </tr>
	       </table>

	       <table>
		<table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/cogsci17/cogsci17_teaser.png" alt="Topics" width="100%" class="border">
		   </td>
		   <td width="80%" valign="center">
		   <papertitle>Inferring Human Interaction from Motion Trajectories in Aerial Videos</papertitle><br/>
	           <a href="https://www.tshu.io/", style="color : #000000;">Tianmin Shu</a>*,
	           <a href="https://yujiapeng.com/", style="color : #000000;">Yujia Peng</a>*,
		   <a href="https://lifengfan.github.io/", style="color : #000000;"><strong>Lifeng Fan</strong></a>,
		   <a href="http://cvl.psych.ucla.edu/", style="color : #000000;">Hongjing Lu</a>,
		   <a href="http://www.stat.ucla.edu/~sczhu/", style="color : #000000;">Song-Chun Zhu</a>
		    </b><br/>
<!-- 		   * Equal contributions  <br /> -->
		   the 39th Annual Meeting of the Cognitive Science Society (CogSci), 2017. <br/>
		   (<font color="red"><strong>Computational Modeling Prize</strong></font>) <br/>
			
		   <a href="files/cogsci17/cogsci.pdf">Paper</a> /
                   <a href="https://www.tshu.io/HeiderSimmel/CogSci17/"> Project </a> /
		   <a href="files/cogsci17/cogsci.bib">Bibtex</a>  <br />
		   </td>
		   </tr>
	       </table>
			
	</article>
</body>
</html> 
